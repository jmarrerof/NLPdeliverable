---
title: "NLP Deliverable - Classification of sarcastic comments in Reddit"
author: "Judith Marrero Ferrera"
date: "30/1/2022"
output: html_document
---

# 1. Import libraries

```{r}
gc() # to ease the memory after using large objects
library(tidyverse)
library(utf8)
library(spacyr)
#library(wordcloud)
library(RColorBrewer)
library(tm)
library(quanteda)
library(quanteda.textplots)
library(quanteda.textmodels)
library(caret)
```

# 2. Load data

```{r}
df <- read.csv("input/train-balanced-sarcasm.csv", encoding = "UTF-8")
```

Check that the dataset is correctly loaded and make a quick overview.

```{r}
head(df)
```

```{r}
dim(df)
```

We can see that the dataset contains 10 columns (attributes, including the label) and a little over 1 million instances. It is the balanced dataset, so both classes 0 and 1 have the same number of comments.

Delete rows with missing values for the "comment" column.

```{r}
df <- na.omit(df) 
sum(df$comment=="")
df <- df[-which(df$comment==""), ]
dim(df)
```

Delete authors for privacy, date attributes and parent comment because the objective is only to analyze the comments themselves. The result is a dataframe of 6 columns containing the comments, subreddit, score, upvotes, downvotes and label.

```{r}
drops <- c("author", "date", "created_utc", "parent_comment")
df <- df[ , !(names(df) %in% drops)]
dim(df)
```

# 3. Exploratory Data Analysis (EDA)

Since the dataset is quite large, we will sample 10k comments of each type (0 - without sarcasm, and 1 - with sarcasm) to make the exploration easier and faster. Also, we will extract the comments to a list.

```{r}
set.seed(123)  # seed for reproducibility
df_nosarc <- df[sample(which(df$label == 0), 10000), ]
df_sarc <- df[sample(which(df$label == 1), 10000), ]

comments_nosarc <- unlist(df_nosarc$comment)
comments_sarc <- unlist(df_sarc$comment)
```

## 3.1 Length of comments

Let's see if there is any length difference between sarcastic and non sarcastic comments.

```{r}
breaks <- seq(from = 0, to = 10000, by = 10)
hist0 <- hist(nchar(comments_nosarc),
              breaks = breaks,
              xlim = c(0, 500))
hist1 <- hist(nchar(comments_sarc),
              breaks = breaks,
              xlim = c(0, 500))

plot(hist0, 
     col=rgb(0,0,1,1/4), 
     xlim = c(0, 500), 
     ylim = c(0, 1500),
     main="Histogram of comment length", 
     xlab="Comment length (number of characters)", 
     ylab="Occurrences") # first histogram
plot(hist1, col=rgb(1,0,0,1/4), add=TRUE)  # second
legend("topright", c("Non-sarcastic", "Sarcastic"), fill=c(rgb(0,0,1,1/4), rgb(1,0,0,1/4)))

```

As seen in the histograms above, sarcastic comments tend to be a little longer than non-sarcastic comments.

## 3.2 Special characters

Sarcastic comments may have more special characters like exclamation or question marks. We will create a function to look for and count them in both sarcastic and non-sarcastic comments.

```{r}
count_character <- function(list, character){
  count = 0
  for (com in 1:length(list)){
    count = count + str_count(list[com], character)
  }
  return(count)
}

# Count exclamation marks
excl_nosarc <- count_character(comments_nosarc, "!")
excl_sarc <- count_character(comments_sarc, "!")

cat("Number of exclamation marks (!) in non-sarcastic comments: ", excl_nosarc)
cat("\nNumber of exclamation marks (!) in sarcastic comments: ", excl_sarc)
```

We can see a clear difference in the numbers. According to them, sarcastic comments have more exclamation marks than non-sarcastic comments, so they can be used to differentiate them.

Now let's try with other special characters:

```{r}
# Count question marks
ques_nosarc <- count_character(comments_nosarc, fixed("?")) # use fixed() to use the literal string
ques_sarc <- count_character(comments_sarc, fixed("?"))

cat("Number of question marks (?) in non-sarcastic comments: ", ques_nosarc)
cat("\nNumber of question marks (?) in sarcastic comments: ", ques_sarc)

# Count asterisks
ast_nosarc <- count_character(comments_nosarc, fixed("*"))
ast_sarc <- count_character(comments_sarc, fixed("*"))

cat("\nNumber of asterisks (*) in non-sarcastic comments: ", ast_nosarc)
cat("\nNumber of asterisks (*) in sarcastic comments: ", ast_sarc)
```

Contrary to exclamation marks, the other special characters tested do not show special frequency differences in sarcastic and non-sarcastic comments. Therefore, exclamations are the only significant difference between both of them that we have discovered so far.

## 3.3 Subreddit analysis

Subreddits are subsidiary threads or categories within the Reddit website. They allow users to focus on a specific interest or topic in posting content. Here, we will analyze the most common subreddits in both groups of comments.

```{r}
# top 10 subreddits in the whole dataset
dplyr::count(df, subreddit, sort = TRUE) %>% head(10)
```

```{r}
# top 10 subreddits in non-sarcastic comments
dplyr::count(df[which(df$label==0), ], subreddit, sort = TRUE) %>% head(10)
```

```{r}
# top 10 subreddits in sarcastic comments
dplyr::count(df[which(df$label==1), ], subreddit, sort = TRUE) %>% head(10)
```

The 10 most frequent subreddits are similar in the two groups, although we can find some interesting differences in the order and the count. They are discussed in the attached report.

## 3.4 Frequent tokens

Now, we are going to tokenize the comments and see which tokens appear the most, both in sarcastic and non-sarcastic comments. To do so, we will initialize spaCyR.

```{r}
spacy_initialize()

tokens_nosarc <- unlist(spacy_tokenize(comments_nosarc))
tokens_sarc <- unlist(spacy_tokenize(comments_sarc))

tokens_nosarc[1:10]
tokens_sarc[1:10]

```

```{r}
cat("Total tokens in non-sarcastic comments: ", length(tokens_nosarc))
cat("\nUnique tokens in non-sarcastic comments: ", length(unique(tokens_nosarc)))

cat("\nTotal tokens in sarcastic comments: ", length(tokens_sarc))
cat("\nUnique tokens in sarcastic comments: ", length(unique(tokens_sarc)))
```

Let's order them by number of appearances and see the most frequent ones.

```{r}
cat("Non-sarcastic: ")
head(sort(table(tokens_nosarc), decreasing=TRUE), n=10)

cat("Sarcastic: ")
head(sort(table(tokens_sarc), decreasing=TRUE), n=10)
```

Actually, most (or all) of them are either punctuation marks or stop words.

## 3.5 Profanity analysis

First, we will download a file containing profane words from the web.

```{r}
# function to get profane words from the web
getProfaneWords <- function() {
    # download profanity file if not done
    profanityFileName <- "profanity.txt"
    if (!file.exists(profanityFileName)) {
        profanity.url <- "https://raw.githubusercontent.com/shutterstock/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/master/en"
        download.file(profanity.url, destfile = profanityFileName, method = "curl")
    }
    
    # load the profanity data if not already in workspace
    if (sum(ls() == "profanity") < 1) {
        profanity <- read.csv(profanityFileName, header = FALSE, stringsAsFactors = FALSE)
        profanity <- profanity$V1
        profanity <- profanity[1:length(profanity)-1]
    }
    
    return(profanity)
}

profaneWords <- dictionary(list(profane_words = getProfaneWords()))
```

Then, we will count the appearances in both sarcastic and non-sarcastic comments. We will use the whole comments (instead of the tokens) because in the list there are complete words and expressions. The analysis was also made with tokens, but the results showed coincidences that did not belonged to profane words or expressions. This chunk can take a while to compute.

```{r}
count_profane <- function(comments, profaneWords) {
  count = 0
  
  # loop through the profane words and comments
  # if there is a coincidence, add 1 to the counter
  for (prof in profaneWords$profane_words){
    for (com in comments){
      # transform comment to lowercase because the profane words are all in lowercase
      count = count + str_count(tolower(com), prof)
    }
  }
  return(count)
}

cat("Profane words in non-sarcastic comments: ", count_profane(comments_nosarc, profaneWords))
cat("\nProfane words in sarcastic comments: ", count_profane(comments_sarc, profaneWords))

```

Non-sarcastic comments have a lower level of profanity than sarcastic comments, because the number of coincidences between the lists is higher in the second case. However, the difference is definitely not that high.

# 4. Preprocessing

First, the dataframe will be converted into a corpus. Then, it will be tokenized to create a document-feature matrix (DFM). The punctuation marks and stop words are eliminated, and all the tokens are transformed to lowercase. This process can take a few seconds.

```{r}
df$comment <- gsub("<.*?>", "", df$comment)
corp <- corpus(df, text_field = "comment")
dfm <- dfm(tokens(corp) %>% tokens_tolower(),
           remove_punct = TRUE) %>% 
  dfm_remove(stopwords('en'))
dfm
```

Check again the most common words (or features) of sarcastic and non-sarcastic comments, now without punctuation marks and stop words, and plotting them in wordclouds.

```{r}
dfm_nosarc <- dfm_subset(dfm, label == 0)
dfm_sarc <- dfm_subset(dfm, label == 1)
```

```{r}
topfeatures(dfm_nosarc)
textplot_wordcloud(dfm_nosarc, max_words = 50)
```

```{r}
topfeatures(dfm_sarc)
textplot_wordcloud(dfm_sarc, max_words = 50)
```

Most of the words are repeated in both word clouds. However, it is interesting that "yeah", "sure", "well" and "obviously" are more important in sarcastic comments and some profane words like "shit" and "fuck" doesn't even appear in the sarcasm word cloud.

Finally, we will create train and test sets (70% - 30%, respectively) to apply the machine learning models.

```{r}
smp_size <- floor(0.7 * nrow(dfm))

# set the seed to make the partition reproducible
set.seed(123)
train_ind <- sample(seq_len(nrow(dfm)), size = smp_size)

dfm_train <- dfm[train_ind, ]
dfm_test <- dfm[-train_ind, ]
```

# 5. Modeling and evaluation

## 5.1 Naive Bayes

First, create the function to perform Naive Bayes classifier and compute metrics. We will try with both multinomial and Bernoulli distributions.

```{r}
# Naive Bayes prediction function from Hands-on 2
nbClassifier <- function(dist){
  # create model
  nbmodel <- textmodel_nb(dfm_train, dfm_train$label, distribution = dist)
  # predictions
  nbpred <- predict(nbmodel, newdata = dfm_test)
  
  # confusion matrix
  confM <- confusionMatrix(table(nbpred, docvars(dfm_test)$label))
  
  # metrics
  acc_coincidences <- sum(as.character(nbpred) == as.character(docvars(dfm_test)$label))
  acc_total <- length(as.character(nbpred))
  accuracy <- acc_coincidences/acc_total
  precision <- confM$byClass['Pos Pred Value']
  recall <- confM$byClass['Sensitivity']
  metrics <- list(accuracy = accuracy, precision = precision, recall = recall)
  
  # build output
  output <- list(metrics=metrics, confM=confM)
  return(output)
}

output_nb <- nbClassifier("multinomial")
metrics_nb <- output_nb$metrics
confM_nb <- output_nb$confM

metrics_nb
confM_nb
```

```{r}
output_nb <- nbClassifier("Bernoulli")
metrics_nb <- output_nb$metrics
confM_nb <- output_nb$confM

metrics_nb
confM_nb
```

Naive Bayes classifier with Bernoulli distribution provides slightly better results than using multinomial distribution. However, neither of them provides a particularly high accuracy, precision or recall. Therefore, we will try with SVM classifier.

## 5.2 SVM

```{r}
# SVM classifier function from Hands-on 2. Since it takes too long to compute (and we get an error), we will use a sample of the original DFM. 
svmClassifier <- function(size, weight){
  
  # subsample of the original dfm
  dfm_sub <- dfm_sample(dfm, size)
  
  # create new train and test sets
  smp_size <- floor(0.7 * nrow(dfm_sub))
  set.seed(123)
  train_ind <- sample(seq_len(nrow(dfm_sub)), size = smp_size)
  dfm_train_sub <- dfm_sub[train_ind, ]
  dfm_test_sub <- dfm_sub[-train_ind, ]
  
  # create model
  svmmodel <- textmodel_svm(dfm_train_sub, 
                          dfm_train_sub$label, 
                          weight = weight)
  # predictions
  svmpred <- predict(svmmodel, newdata = dfm_test_sub)
  
  # confusion matrix
  confM <- confusionMatrix(table(svmpred, docvars(dfm_test_sub)$label))
  
  # metrics
  acc_coincidences <- sum(as.character(svmpred) == as.character(docvars(dfm_test_sub)$label))
  acc_total <- length(as.character(svmpred))
  accuracy <- acc_coincidences/acc_total
  precision <- confM$byClass['Pos Pred Value']
  recall <- confM$byClass['Sensitivity']
  metrics <- list(accuracy = accuracy, precision = precision, recall = recall)
  
  # build output
  output <- list(metrics=metrics, confM=confM)
  return(output)
}

output_svm <- svmClassifier(10000, "uniform")

metrics_svm <- output_svm$metrics
confM_svm <- output_svm$confM

metrics_svm
confM_svm
```

These results are similar or even worse, but maybe it is because the size of the sample is too small. We will increase it and see if the metrics improve. This section will take a while to compute.

```{r}
acc_vec <- c()
prec_vec <- c()
rec_vec <- c()

# 20000 was the maximum size without getting a memory error. It can be changed if using other resources.
sizes <- seq(from = 5000, to = 20000, by = 5000)

for (i in sizes){
  # compute models with different sizes
  output_svm <- svmClassifier(i, "uniform")
  
  # add results to our lists
  acc_vec <- c(acc_vec, output_svm$metrics$accuracy)
  prec_vec <- c(prec_vec, output_svm$metrics$precision)
  rec_vec <- c(rec_vec, output_svm$metrics$recall)
}
```

```{r}
# plot results
plot(x = sizes, y = acc_vec, 
     type = "l",
     col = "blue",
     xlim = c(5000, 20000),
     ylim = c(0,1),
     main = "Classification metrics with different sample sizes",
     xlab = "Sample size",
     ylab = "Score")
lines(x = sizes, y = prec_vec,
      col = "green")
lines(x = sizes, y = rec_vec,
      col = "red")
legend("topright",
       c("Accuracy", "Precision", "Recall"), 
       fill = c("blue", "green", "red"))
```

Finalize...

```{r}
spacy_finalize()
```
